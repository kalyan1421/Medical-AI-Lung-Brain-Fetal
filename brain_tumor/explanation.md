ğŸ§  Brain Tumor Classification Using Deep Learning

Multi-Class MRI Image Classification with EfficientNetV2S

ğŸ“– 1. Project Overview

This project implements a state-of-the-art deep learning pipeline to classify brain MRI scans into four clinically relevant categories:

Class	Description
Glioma	Malignant tumor originating from glial cells
Meningioma	Tumor arising from the meninges (brain covering)
No Tumor	Healthy brain MRI with no abnormality
Pituitary Tumor	Tumor affecting the pituitary gland

The system is designed using transfer learning, advanced regularization, and a three-stage fine-tuning strategy to achieve high accuracy and strong generalization, making it suitable for medical AI applications.

ğŸ—ï¸ 2. Model Architecture
ğŸ”¹ High-Level Architecture
Input Image (224 Ã— 224 Ã— 3)
        â”‚
        â–¼
EfficientNetV2S Backbone (ImageNet pre-trained)
        â”‚
        â–¼
Dual Pooling Layer
(Global Average Pooling + Global Max Pooling)
        â”‚
        â–¼
Fully Connected Classification Head
(BatchNorm + Dropout + Dense layers)
        â”‚
        â–¼
Softmax Output (4 classes)

ğŸ”¹ Key Architectural Choices

EfficientNetV2S

~21M parameters

Compound scaling (depth, width, resolution)

Fused-MBConv blocks for efficiency

Optimized for faster convergence

Dual Pooling

Captures both:

Global feature presence (Average Pooling)

Strongest activations (Max Pooling)

Produces a richer feature representation

Regularized Dense Head

Swish activation

L2 weight regularization

Dropout (0.3â€“0.4)

Batch Normalization

ğŸ§  3. Algorithms & Techniques Used
3.1 Transfer Learning

The backbone network is pre-trained on ImageNet (14+ million images), allowing the model to reuse learned visual features such as:

Edge detection

Texture patterns

Shape recognition

Benefits

Faster training

Requires less medical data

Improved accuracy and stability

3.2 Three-Stage Training Strategy
Stage	Configuration	Purpose
Stage 1	Backbone frozen, head trained (LR = 1e-3, 25 epochs)	Learn task-specific features
Stage 2	Top 30% backbone unfrozen (LR = 1e-4, 35 epochs)	Adapt high-level representations
Stage 3	Full model fine-tuning (LR = 5e-5, 15 epochs)	Fine-grained optimization

Why this approach?

Prevents catastrophic forgetting

Gradual domain adaptation

Preserves pre-trained knowledge

3.3 Learning Rate Strategy

Cosine Annealing with Warmup

Warmup phase: stabilizes early training

Cosine decay: smooth convergence and fine tuning

Mathematically:

Warmup:
LR = initial_lr Ã— (epoch + 1) / warmup_epochs

Cosine Decay:
LR = min_lr + (initial_lr âˆ’ min_lr) Ã— (1 + cos(Ï€ Ã— progress)) / 2

3.4 Label Smoothing

Instead of hard one-hot labels:

[1, 0, 0, 0]


Smoothed labels:

[0.925, 0.025, 0.025, 0.025]


Advantages

Reduces overconfidence

Improves generalization

Acts as implicit regularization

3.5 Class Imbalance Handling

Class weights are computed as:

weight_i = total_samples / (num_classes Ã— class_samples_i)

Class	Samples	Weight
Glioma	1321	1.08
Meningioma	1339	1.07
No Tumor	1595	0.90
Pituitary	1457	0.98

This prevents bias toward majority classes.

ğŸ§ª 4. Data Augmentation Strategy

To improve robustness and generalization:

Rotation (Â±15Â°)

Width & height shift (Â±10%)

Zoom (Â±10%)

Horizontal flip

Brightness adjustment (85â€“115%)

Shear (Â±10%)

âŒ Vertical flipping is avoided to preserve anatomical orientation.

ğŸ” 5. Test-Time Augmentation (TTA)

Each test image is evaluated multiple times with slight transformations.
Final prediction = average of all predictions

Benefits

Reduces prediction variance

Improves accuracy by ~1â€“3%

ğŸ“Š 6. Dataset Pipeline
Directory Structure
cleaned_data/
â”œâ”€â”€ Training/
â”‚   â”œâ”€â”€ glioma/
â”‚   â”œâ”€â”€ meningioma/
â”‚   â”œâ”€â”€ notumor/
â”‚   â””â”€â”€ pituitary/
â”‚
â””â”€â”€ Testing/
    â”œâ”€â”€ glioma/
    â”œâ”€â”€ meningioma/
    â”œâ”€â”€ notumor/
    â””â”€â”€ pituitary/

Dataset Split
Split	Images
Training	4,855
Validation	857
Testing	1,311
ğŸ”§ 7. Regularization Techniques

L2 Weight Regularization (Î» = 1e-4)

Dropout (0.3â€“0.4)

Batch Normalization

Label Smoothing

Data Augmentation

Early Stopping (patience = 8â€“12 epochs)

ğŸ“ˆ 8. Training Callbacks
Callback	Purpose
LearningRateScheduler	Cosine decay with warmup
EarlyStopping	Stop on validation plateau
ModelCheckpoint	Save best model
ReduceLROnPlateau	Adaptive LR reduction
ğŸ“Š 9. Evaluation Metrics

Accuracy

Precision (per class)

Recall (per class)

AUC (ROC)

Confusion Matrix

ğŸ”„ 10. End-to-End Training Flow
Dataset Cleaning
      â†“
Data Augmentation
      â†“
Class Weight Calculation
      â†“
Model Construction
      â†“
Stage 1 Training
      â†“
Stage 2 Fine-Tuning
      â†“
Stage 3 Fine-Tuning
      â†“
Best Model Selection
      â†“
Test Evaluation (with TTA)
      â†“
Model & Report Saving

ğŸ’¾ 11. Output Artifacts
models/
â”œâ”€â”€ stage1_best.h5
â”œâ”€â”€ brain_tumor_best.h5
â”œâ”€â”€ brain_tumor_final.h5
â”œâ”€â”€ brain_tumor_model.h5
â””â”€â”€ class_labels.json

results/
â”œâ”€â”€ test_results_final.json
â””â”€â”€ training_results_final.png

ğŸš€ 12. How to Run
# Step 1: Dataset Cleaning
python 1_clean_dataset.py

# Step 2: Model Training
python 2_train_model.py

# Step 3: Model Evaluation
python 3_test_model.py

ğŸ“Š 13. Expected Performance
Metric	Expected Value
Overall Accuracy	93â€“97%
Per-Class Accuracy	>90%
Precision	>92%
Recall	>92%
AUC	>0.98
ğŸ Conclusion

This project demonstrates a production-grade medical imaging pipeline using:

Advanced transfer learning

Multi-stage fine-tuning

Robust regularization

Test-time augmentation

The approach significantly outperforms basic CNN and naÃ¯ve transfer learning methods, making it highly suitable for real-world healthcare AI applications.





======================================================================
ğŸ“Š EVALUATING MODEL
======================================================================
82/82 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12s 91ms/step - accuracy: 0.9184 - auc: 0.9902 - loss: 0.5856 - precision: 0.9246 - recall: 0.8986 

ğŸ“Š Standard Evaluation:
  Loss:      0.5856
  Accuracy:  91.84%
  Precision: 92.46%
  Recall:    89.86%
  AUC:       0.9902

ğŸ”® Predicting with TTA (5 augmentations)...
Found 1311 images belonging to 4 classes.
Found 1311 images belonging to 4 classes.
Found 1311 images belonging to 4 classes.
Found 1311 images belonging to 4 classes.

ğŸ“Š TTA Accuracy: 91.91%

ğŸ“‹ Classification Report:
              precision    recall  f1-score   support

      glioma     0.9265    0.8400    0.8811       300
  meningioma     0.8388    0.8333    0.8361       306
     notumor     0.9619    0.9975    0.9794       405
   pituitary     0.9333    0.9800    0.9561       300

    accuracy                         0.9191      1311
   macro avg     0.9151    0.9127    0.9132      1311
weighted avg     0.9185    0.9191    0.9181      1311


ğŸ“Š Per-Class Accuracy:
  glioma: 84.00%
  meningioma: 83.33%
  notumor: 99.75%
  pituitary: 98.00%